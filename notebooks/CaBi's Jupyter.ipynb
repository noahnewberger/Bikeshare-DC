{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CaBI Data Downloading and Simple Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The first seciton uses a chrome browser to download all the zip files from CaBI site to a local directory. Then unpacks all the zip files to the same directory. Within these files is the trip history by bike, including starting and ending station, duration of the trip, member type, date/time of trip.\n",
    "\n",
    "The second section downloads a json page of all the station locations and their region ID. Then is downloads another json page that maps the region IDs to the DMV regions as defined by CaBi.\n",
    "\n",
    "The third section preprocess the raw csv files\n",
    "    -extracts the day of the week, hour, and day for each ride\n",
    "    -megres the region codes into the trip data. \n",
    "    -converts duration to minutes from miliseconds\n",
    "    -stacks the qarterly CSVs into years\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../cabi_data\\2010_trip_history_data.zip\n",
      "../cabi_data\\2011_trip_history_data.zip\n",
      "../cabi_data\\2012_trip_history_data.zip\n",
      "../cabi_data\\2013_trip_history_data.zip\n",
      "../cabi_data\\2014_trip_history_data.zip\n",
      "../cabi_data\\2015_trip_history_data.zip\n",
      "../cabi_data\\2016_trip_history_data.zip\n",
      "../cabi_data\\2017_trip_history_data.zip\n"
     ]
    }
   ],
   "source": [
    "### download_all_cabi.py\n",
    "\n",
    "from splinter import Browser\n",
    "import os\n",
    "import urllib.request\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "\n",
    "def get_links():\n",
    "    ### You'll need to find a chrome browser or firefox browser to launch this function\n",
    "    browser = Browser('chrome')\n",
    "    browser.visit('https://s3.amazonaws.com/capitalbikeshare-data/index.html')\n",
    "\n",
    "    links = browser.find_by_css('#tbody-content a')\n",
    "    #links = [link for link in links if link['href']]\n",
    "    zip_links = []\n",
    "    for link in links:\n",
    "        link_text = link['href']\n",
    "        with urllib.request.urlopen(link_text) as response:\n",
    "            subtype = response.info().get_content_subtype()\n",
    "            if subtype == 'zip':\n",
    "                zip_links.append(link_text)\n",
    "\n",
    "    return zip_links\n",
    "\n",
    "\n",
    "def download_links(links):\n",
    "    download_dir = '../cabi_data'\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.mkdir(download_dir)\n",
    "    for link in links:\n",
    "        file_name = os.path.join(download_dir, link.split('/')[-1])\n",
    "        print(file_name)\n",
    "        # save file to disk\n",
    "        with urllib.request.urlopen(link) as response, open(file_name, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response, out_file)\n",
    "        with zipfile.ZipFile(file_name, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(download_dir)\n",
    "\n",
    "links = get_links()\n",
    "download_links(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490\n",
      "Washington, DC                   269\n",
      "Arlington, VA                     92\n",
      "Montgomery County, MD (South)     45\n",
      "Alexandria, VA                    31\n",
      "Montgomery County, MD (North)     28\n",
      "Fairfax, VA                       25\n",
      "Name: region_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "####CaBi_Stations\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load station information\n",
    "station_url = \"https://gbfs.capitalbikeshare.com/gbfs/en/station_information.json\"\n",
    "stations = requests.get(station_url).json()\n",
    "station_df = pd.DataFrame(stations['data']['stations'])\n",
    "station_df = station_df[['lat', 'lon', 'region_id', 'short_name']].copy()\n",
    "# Default any missing region ids to DC (only example as of 2/21 is new station at Anacostia Park)\n",
    "station_df['region_id'] = np.where(\n",
    "    station_df['region_id'].isnull(), 42, station_df['region_id'])\n",
    "\n",
    "# Convert region_id to str from float\n",
    "station_df['region_id'] = station_df['region_id'].astype(int).astype(str)\n",
    "\n",
    "# Load region information\n",
    "region_url = \"https://gbfs.capitalbikeshare.com/gbfs/en/system_regions.json\"\n",
    "regions = requests.get(region_url).json()\n",
    "regions_df = pd.DataFrame(regions['data']['regions'])\n",
    "\n",
    "# Merge region information onto stations\n",
    "station_df = station_df.merge(\n",
    "    regions_df, left_on='region_id', right_on='region_id', how='left')\n",
    "station_df.rename(index=str, columns={'name': 'region_name'}, inplace=True)\n",
    "print(len(station_df))\n",
    "print(station_df['region_name'].value_counts())\n",
    "\n",
    "# Define Abbreviations for each region\n",
    "\n",
    "region_code = {'Washington, DC': 'WDC',\n",
    "               'Arlington, VA': 'ARL',\n",
    "               'Montgomery County, MD (South)': 'MCS',\n",
    "               'Montgomery County, MD (North)': 'MCN',\n",
    "               'Alexandria, VA': 'ALX',\n",
    "               'Fairfax, VA': 'FFX'}\n",
    "\n",
    "region_code_series = pd.Series(region_code, name='region_code')\n",
    "region_code_series.index.name = 'region_name'\n",
    "region_code_df = region_code_series.reset_index()\n",
    "station_df = station_df.merge(\n",
    "    region_code_df, left_on='region_name', right_on='region_name', how='left')\n",
    "station_df.drop(['region_name', 'region_id'], inplace=True, axis=1)\n",
    "\n",
    "# Output DataFrame as CSV with timestamp\n",
    "TIMESTR = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = \"CABI_Station_Info_\" + TIMESTR + \".csv\"\n",
    "filepath = os.path.join(\"C:/Users/Noah/\", filename)\n",
    "station_df.to_csv(filepath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Noah/cabi_data\\2010-q4_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2011-q1_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2011-q2_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2011-q3_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2011-q4_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2012-q1_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2012-q2_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2012-q3_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2012-q4_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2013-q1_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2013-q2_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2013-q3_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2013-q4_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2014-q1_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2014-q2_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2014-q3_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2014-q4_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2015-q1_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2015-q2_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2015-q3_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2015-q4_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2016-q1_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2016-q2_trip_history_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (3,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Noah/cabi_data\\2016-q3_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2016-q4_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2017-q1_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2017-q2_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2017-q3_trip_history_data.csv\n",
      "C:/Users/Noah/cabi_data\\2017-q4_trip_history_data.csv\n"
     ]
    }
   ],
   "source": [
    "## Compile Bikeshare Data ##\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from datetime import date\n",
    "import calendar\n",
    "\n",
    "\n",
    "TIMESTR = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Load Cabi Trip Data as Data Frame\n",
    "\n",
    "Final_Daily_Rides = pd.DataFrame()\n",
    "\n",
    "all_header_files = glob2.glob('C:/Users/Noah/cabi_data//*.csv')\n",
    "FILE_LIST = []\n",
    "FILE_LIST_all = []\n",
    "counter = 0\n",
    "for file in all_header_files:\n",
    "    counter = counter +1 \n",
    "    FILE = re.search('([2][0][0-9][0-9][-][a-z][0-4])',file)\n",
    "    FILE_LIST.append(FILE.group(0))\n",
    "    FILE_LIST_all.append(FILE.group(0))\n",
    "    trip_df = pd.read_csv(file)\n",
    "    trip_df['source'] = FILE\n",
    "    # Derive Trip Date from Start Date time stamp\n",
    "    start_datetime = pd.to_datetime(trip_df['Start date'])\n",
    "    end_datetime = pd.to_datetime(trip_df['End date'])\n",
    "    trip_df['start_date'] = start_datetime.map(lambda x: x.date())\n",
    "    trip_df['end_date'] = end_datetime.map(lambda x: x.date())\n",
    "    #0-23, Military Time - EST\n",
    "    trip_df['start_hour'] = start_datetime.dt.hour\n",
    "    trip_df['end_hour'] = end_datetime.dt.hour\n",
    "    #0-6, 0 is Monday - 6 is Sunday\n",
    "    trip_df['Weekday'] = trip_df['start_date'].apply(lambda x: x.weekday())\n",
    "    # Append Region Code onto Trip Data\n",
    "    station_info_df = pd.read_csv('C:/Users/Noah/CABI_Station_Info_20180307_080242.csv')\n",
    "    start_station_info_df = station_info_df.add_prefix('start_')\n",
    "    trip_df = trip_df.merge(start_station_info_df, left_on='Start station number',\n",
    "                            right_on='start_short_name', how='left')\n",
    "    end_station_info_df = station_info_df.add_prefix('end_')\n",
    "    trip_df = trip_df.merge(end_station_info_df, left_on='End station number',\n",
    "                            right_on='end_short_name', how='left')\n",
    "    trip_df['region_start_end'] = trip_df['start_region_code'] + \\\n",
    "        '_to_' + trip_df['end_region_code']\n",
    "    trip_df['station_start_end'] = trip_df['Start station'] + \\\n",
    "        '_to_' + trip_df['End station']\n",
    "    # Milisecond per Minute 0.0000166667 \n",
    "    trip_df['Minutes'] = 0.0000166667 * trip_df['Duration (ms)']\n",
    "    '''There are a handful of trips that will be dropped\n",
    "    because they are either warehouse trips or the station\n",
    "    no longer exists,option to find location by name instead of\n",
    "    station information'''\n",
    "    # print(trip_df[(trip_df['region_start_end'].isnull()) & (\n",
    "    # ~trip_df['station_start_end'].str.contains('6035 Warehouse'))]['station_start_end'].value_counts())\n",
    "    \n",
    "    #Append to Final Dataframe\n",
    "    Final_Daily_Rides = Final_Daily_Rides.append(trip_df)\n",
    "    if counter == 4:\n",
    "        final = '_'.join(str(v) for v in FILE_LIST)\n",
    "        filename = \"CABI_Daily_Trips_\" + TIMESTR + \"_\" + str(final) + \".csv\"\n",
    "        filepath = os.path.join(\"C:/Users/Noah/Bikeshare-DC\", filename)\n",
    "        Final_Daily_Rides.to_csv(filepath, index=False)\n",
    "        FILE_LIST =[]\n",
    "        Final_Daily_Rides =pd.DataFrame()\n",
    "        counter = 0\n",
    "    elif file == all_header_files[0]:\n",
    "        final = '_'.join(str(v) for v in FILE_LIST)\n",
    "        filename = \"CABI_Daily_Trips_\" + TIMESTR + \"_\" + str(final) + \".csv\"\n",
    "        filepath = os.path.join(\"C:/Users/Noah/Bikeshare-DC\", filename)\n",
    "        Final_Daily_Rides.to_csv(filepath, index=False)\n",
    "        FILE_LIST =[]\n",
    "        Final_Daily_Rides =pd.DataFrame()\n",
    "        counter = 0\n",
    "    elif len(FILE_LIST_all) == len(all_header_files):\n",
    "        final = '_'.join(str(v) for v in FILE_LIST)\n",
    "        filename = \"CABI_Daily_Trips_\" + TIMESTR + \"_\" + str(final) + \".csv\"\n",
    "        filepath = os.path.join(\"C:/Users/Noah/Bikeshare-DC\", filename)\n",
    "        Final_Daily_Rides.to_csv(filepath, index=False)\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    \n",
    "#Weird Error for 2016-q3 File\n",
    "#C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (3,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
    "#interactivity=interactivity, compiler=compiler, result=result)\n",
    "#C:/Users/Noah/cabi_data\\2016-q3_trip_history_data.csv\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
